{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Z5O71OuoPd23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a502602-545c-4c2f-eaff-d85969651662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Personal Color 분류 CLIP 3 - CoCoOp\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install ftfy regex tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import clip\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from typing import List, Tuple, Optional"
      ],
      "metadata": {
        "id": "gQDPj8zXQIvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f97ef8c-1bdb-447e-c117-56c1bb4a8fa3"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fsjkvukw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-fsjkvukw\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, classnames, clip_model, device, n_ctx=16, ctx_init=None):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.n_classes = len(classnames)\n",
        "        self.ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        self.n_ctx = n_ctx\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "        # Improved Meta-Net with Layer Normalization and Residual Connection\n",
        "        self.meta_net = nn.Sequential(\n",
        "            nn.LayerNorm(self.ctx_dim),\n",
        "            nn.Linear(self.ctx_dim, self.ctx_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(self.ctx_dim * 4, self.ctx_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Improved Context Initialization\n",
        "        if ctx_init is None:\n",
        "            ctx_vectors = []\n",
        "            for name in classnames:\n",
        "                with torch.no_grad():\n",
        "                    tokens = clip.tokenize(f\"a photo of a person with {name} color tone\").to(device)\n",
        "                    ctx_vector = clip_model.token_embedding(tokens[0]).detach()\n",
        "                    ctx_vectors.append(ctx_vector)\n",
        "            ctx_init = torch.stack(ctx_vectors).mean(dim=0)[:self.n_ctx]\n",
        "\n",
        "        # Add positional embeddings to context initialization\n",
        "        pos_embeddings = torch.arange(self.n_ctx).float().to(device)\n",
        "        pos_embeddings = pos_embeddings / self.n_ctx\n",
        "        pos_embeddings = pos_embeddings.unsqueeze(1).expand(-1, self.ctx_dim)\n",
        "        ctx_init = ctx_init + 0.1 * pos_embeddings\n",
        "\n",
        "        self.ctx = nn.Parameter(ctx_init)\n",
        "\n",
        "        # Improved prompt construction\n",
        "        prompt_templates = [\n",
        "            f\"a photo of a person with {name} color tone\",\n",
        "            f\"a portrait showing {name} seasonal color characteristics\",\n",
        "            f\"an image demonstrating {name} personal color features\"\n",
        "        ]\n",
        "\n",
        "        # Use the first prompt template as the base\n",
        "        template = prompt_templates[0]\n",
        "        self.tokenized_prompts = torch.cat([clip.tokenize(template.format(name=name)) for name in classnames]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(self.tokenized_prompts).type(self.dtype)\n",
        "\n",
        "        self.register_buffer('embedding', embedding)\n",
        "        self.prompt_prefix_length = 4\n",
        "        self.name_length = embedding.size(1) - self.prompt_prefix_length\n",
        "\n",
        "    def forward(self, batch_size):\n",
        "        # Dynamic context generation with residual connection\n",
        "        ctx_features = self.meta_net(self.ctx)\n",
        "        ctx = self.ctx + ctx_features  # Residual connection\n",
        "\n",
        "        # Add batch dimension for processing\n",
        "        ctx = ctx.unsqueeze(0)  # Shape: [1, n_ctx, ctx_dim]\n",
        "\n",
        "        # Multi-scale context features\n",
        "        ctx_scales = [ctx]\n",
        "        for i in range(2):  # Generate 3 scales\n",
        "            # Correct permutation for 1D pooling\n",
        "            ctx_permuted = ctx.permute(0, 2, 1)  # Shape: [1, ctx_dim, n_ctx]\n",
        "            ctx_scaled = F.adaptive_avg_pool1d(ctx_permuted, ctx.size(1) // (2 ** (i+1)))\n",
        "            ctx_scaled = F.interpolate(ctx_scaled, size=ctx.size(1), mode='linear')\n",
        "            ctx_scales.append(ctx_scaled.permute(0, 2, 1))  # Shape: [1, n_ctx, ctx_dim]\n",
        "\n",
        "        # Average the multi-scale features\n",
        "        ctx = torch.stack(ctx_scales).mean(dim=0)  # Shape: [1, n_ctx, ctx_dim]\n",
        "        ctx = ctx.expand(batch_size * self.n_classes, -1, -1)\n",
        "\n",
        "        # Process name embeddings\n",
        "        name_embeddings = self.embedding[:, self.prompt_prefix_length:, :]\n",
        "        name_embeddings = name_embeddings.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
        "        name_embeddings = name_embeddings.reshape(-1, self.name_length, self.ctx_dim)\n",
        "\n",
        "        # Concatenate context and name embeddings\n",
        "        prompts = torch.cat([ctx, name_embeddings], dim=1)\n",
        "\n",
        "        # Handle prompt length\n",
        "        if prompts.size(1) > 77:\n",
        "            prompts = prompts[:, :77, :]\n",
        "        else:\n",
        "            padding = torch.zeros(\n",
        "                prompts.size(0),\n",
        "                77 - prompts.size(1),\n",
        "                prompts.size(2),\n",
        "                dtype=self.dtype,\n",
        "                device=self.device\n",
        "            )\n",
        "            prompts = torch.cat([prompts, padding], dim=1)\n",
        "\n",
        "        return prompts"
      ],
      "metadata": {
        "id": "bCYh7ItO1uuk"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, classnames, clip_model, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # Initialize model parameters in float32 for training stability\n",
        "        self.dtype = torch.float32\n",
        "        self.clip_dtype = clip_model.dtype  # Store original CLIP dtype for inference\n",
        "\n",
        "        self.prompt_learner = PromptLearner(classnames, clip_model, device)\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = clip_model.transformer\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "\n",
        "        # Configure fusion components\n",
        "        hidden_dim = clip_model.visual.output_dim\n",
        "        self.fusion = nn.ModuleDict({\n",
        "            'attention': nn.MultiheadAttention(\n",
        "                embed_dim=hidden_dim,\n",
        "                num_heads=8,\n",
        "                dropout=0.1,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            'norm1': nn.LayerNorm(hidden_dim),\n",
        "            'norm2': nn.LayerNorm(hidden_dim),\n",
        "            'mlp': nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(hidden_dim * 4, hidden_dim),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "        })\n",
        "\n",
        "        # Convert all modules to float32\n",
        "        self.to(dtype=torch.float32)\n",
        "\n",
        "    def forward(self, image):\n",
        "        batch_size = image.size(0)\n",
        "\n",
        "        # Ensure input image is in float32\n",
        "        image = image.to(device=self.device, dtype=torch.float32)\n",
        "\n",
        "        # Process image features\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float32):\n",
        "            image_features = self.image_encoder(image)\n",
        "            image_features = image_features.to(dtype=torch.float32)\n",
        "\n",
        "            # Prepare for attention\n",
        "            image_features = image_features.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
        "\n",
        "            # Apply attention\n",
        "            attn_output, _ = self.fusion['attention'](\n",
        "                image_features, image_features, image_features\n",
        "            )\n",
        "\n",
        "            # Residual connection and normalization\n",
        "            image_features = image_features + attn_output\n",
        "            image_features = self.fusion['norm1'](image_features)\n",
        "\n",
        "            # MLP with residual connection\n",
        "            mlp_output = self.fusion['mlp'](image_features)\n",
        "            image_features = image_features + mlp_output\n",
        "            image_features = self.fusion['norm2'](image_features)\n",
        "\n",
        "            # Remove sequence dimension and normalize\n",
        "            image_features = image_features.squeeze(1)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Process text features\n",
        "            prompts = self.prompt_learner(batch_size)\n",
        "            x = prompts + self.positional_embedding.type(torch.float32)\n",
        "            x = x.permute(1, 0, 2)\n",
        "            x = self.text_encoder(x)\n",
        "            x = x.permute(1, 0, 2)\n",
        "            x = self.ln_final(x)\n",
        "\n",
        "            # Extract and normalize text features\n",
        "            text_features = x[:, -1, :] @ self.text_projection.type(torch.float32)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Compute logits\n",
        "            image_features = image_features.unsqueeze(1).expand(-1, self.prompt_learner.n_classes, -1)\n",
        "            image_features = image_features.reshape(-1, image_features.size(-1))\n",
        "\n",
        "            logit_scale = self.logit_scale.exp()\n",
        "            logits = logit_scale * torch.sum(image_features * text_features, dim=-1)\n",
        "            logits = logits.view(batch_size, self.prompt_learner.n_classes)\n",
        "\n",
        "            return logits"
      ],
      "metadata": {
        "id": "Rn0N_HJt1uqB"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PersonalColorDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_paths: List[str],\n",
        "        labels: List[int],\n",
        "        transform: Optional[transforms.Compose] = None,\n",
        "        augment: bool = False\n",
        "    ):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "\n",
        "        # Improvement 5: Enhanced color augmentation\n",
        "        if augment:\n",
        "            self.color_aug = transforms.Compose([\n",
        "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "                transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
        "                transforms.RandomAutocontrast(p=0.5)\n",
        "            ])\n",
        "        else:\n",
        "            self.color_aug = None\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            if self.augment and np.random.random() > 0.5:\n",
        "                image = self.color_aug(image)\n",
        "\n",
        "        return image, self.labels[idx]"
      ],
      "metadata": {
        "id": "il-IhNN_7szL"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    # Configure optimizer\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': [p for n, p in model.prompt_learner.named_parameters() if \"meta_net\" not in n], 'lr': 2e-3},\n",
        "        {'params': model.prompt_learner.meta_net.parameters(), 'lr': 1e-3},\n",
        "        {'params': model.fusion.parameters(), 'lr': 5e-4}\n",
        "    ], weight_decay=0.05)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=num_epochs // 3,\n",
        "        T_mult=2,\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    criterion = criterion.to(device=device)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Use float32 for training\n",
        "            with torch.amp.autocast('cuda', dtype=torch.float32):\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        val_acc = validate_with_tta(model, val_loader, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
        "        print(f'Validation Accuracy: {val_acc:.4f}')"
      ],
      "metadata": {
        "id": "1pPnqZh_69Sa"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_with_tta(model, val_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "\n",
        "            # Test-time augmentation\n",
        "            tta_outputs = []\n",
        "\n",
        "            # Use float32 for validation as well\n",
        "            with torch.amp.autocast('cuda', dtype=torch.float32):\n",
        "                for flip in [False, True]:\n",
        "                    img = images.flip(3) if flip else images\n",
        "                    for scale in [0.9, 1.0, 1.1]:\n",
        "                        size = int(224 * scale)\n",
        "                        if size != 224:\n",
        "                            img_scaled = F.interpolate(img, size=(size, size),\n",
        "                                                     mode='bilinear',\n",
        "                                                     align_corners=False)\n",
        "                            img_scaled = F.interpolate(img_scaled, size=(224, 224),\n",
        "                                                     mode='bilinear',\n",
        "                                                     align_corners=False)\n",
        "                        else:\n",
        "                            img_scaled = img\n",
        "\n",
        "                        outputs = model(img_scaled)\n",
        "                        tta_outputs.append(outputs)\n",
        "\n",
        "            # Average predictions\n",
        "            outputs = torch.stack(tta_outputs).mean(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "nHuM38U7BOxa"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    dataset_dir = '/content/drive/Othercomputers/내 노트북/personal-color-data/'\n",
        "    dataset_types = ['train', 'test']\n",
        "    class_folders = ['spring', 'summer', 'fall', 'winter']\n",
        "\n",
        "    image_paths = {'train': [], 'test': []}\n",
        "    labels = {'train': [], 'test': []}\n",
        "\n",
        "    for dataset_type in dataset_types:\n",
        "        for idx, class_folder in enumerate(class_folders):\n",
        "            class_dir = os.path.join(dataset_dir, dataset_type, class_folder)\n",
        "            for img_path in glob.glob(os.path.join(class_dir, '*.*')):\n",
        "                if img_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_paths[dataset_type].append(img_path)\n",
        "                    labels[dataset_type].append(idx)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "    model = CustomCLIP(class_folders, clip_model, device).to(device)\n",
        "\n",
        "    # 개선 9: Strong augmentation for training\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.RandomApply([transforms.ColorJitter(0.3, 0.3, 0.3, 0.1)], p=0.5),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                           std=(0.26862954, 0.26130258, 0.27577711))\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                           std=(0.26862954, 0.26130258, 0.27577711))\n",
        "    ])\n",
        "\n",
        "    train_dataset = PersonalColorDataset(image_paths['train'], labels['train'],\n",
        "                                       transform=train_transform, augment=True)\n",
        "    val_dataset = PersonalColorDataset(image_paths['test'], labels['test'],\n",
        "                                     transform=val_transform, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                            num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, num_epochs=15, device=device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xQv8I5T1ufE",
        "outputId": "9c1302e2-31ed-4c39-a0f4-efc879b4949a"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 2/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 3/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 4/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 5/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 6/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 7/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 8/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 9/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 10/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 11/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 12/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 13/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 14/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n",
            "Epoch 15/15:\n",
            "Training Loss: 1.3863\n",
            "Validation Accuracy: 0.2284\n"
          ]
        }
      ]
    }
  ]
}