{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Z5O71OuoPd23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa6d875-5683-4777-a0df-eb9b739b6bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Personal Color 분류 CLIP 3 - CoCoOp\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install ftfy regex tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import clip\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from typing import List, Tuple, Optional"
      ],
      "metadata": {
        "id": "gQDPj8zXQIvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0be571-f8bd-4001-f211-905318180613"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-u4a68y93\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-u4a68y93\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, classnames, clip_model, device, n_ctx=16, ctx_init=None):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.n_classes = len(classnames)\n",
        "        self.ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        self.n_ctx = n_ctx\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "        # Improved Meta-Net with Layer Normalization and Residual Connection\n",
        "        self.meta_net = nn.Sequential(\n",
        "            nn.LayerNorm(self.ctx_dim),\n",
        "            nn.Linear(self.ctx_dim, self.ctx_dim * 4),\n",
        "            nn.GELU(),  # Changed from ReLU to GELU\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(self.ctx_dim * 4, self.ctx_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Improved Context Initialization\n",
        "        if ctx_init is None:\n",
        "            ctx_vectors = []\n",
        "            for name in classnames:\n",
        "                with torch.no_grad():\n",
        "                    tokens = clip.tokenize(f\"a photo of a person with {name} color tone\").to(device)\n",
        "                    ctx_vector = clip_model.token_embedding(tokens[0]).detach()\n",
        "                    ctx_vectors.append(ctx_vector)\n",
        "            ctx_init = torch.stack(ctx_vectors).mean(dim=0)[:self.n_ctx]\n",
        "\n",
        "        # Add positional embeddings to context initialization\n",
        "        pos_embeddings = torch.arange(self.n_ctx).float().to(device)\n",
        "        pos_embeddings = pos_embeddings / self.n_ctx\n",
        "        pos_embeddings = pos_embeddings.unsqueeze(1).expand(-1, self.ctx_dim)\n",
        "        ctx_init = ctx_init + 0.1 * pos_embeddings\n",
        "\n",
        "        self.ctx = nn.Parameter(ctx_init)\n",
        "\n",
        "        # Improved prompt construction\n",
        "        prompt_templates = [\n",
        "            f\"a photo of a person with {name} color tone\",\n",
        "            f\"a portrait showing {name} seasonal color characteristics\",\n",
        "            f\"an image demonstrating {name} personal color features\"\n",
        "        ]\n",
        "\n",
        "        # Use the first prompt template as the base\n",
        "        template = prompt_templates[0]\n",
        "        self.tokenized_prompts = torch.cat([clip.tokenize(template.format(name=name)) for name in classnames]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(self.tokenized_prompts).type(self.dtype)\n",
        "\n",
        "        self.register_buffer('embedding', embedding)\n",
        "        self.prompt_prefix_length = 4\n",
        "        self.name_length = embedding.size(1) - self.prompt_prefix_length\n",
        "\n",
        "    def forward(self, batch_size):\n",
        "        # Dynamic context generation with residual connection\n",
        "        ctx_features = self.meta_net(self.ctx)\n",
        "        ctx = self.ctx + ctx_features  # Residual connection\n",
        "\n",
        "        # Multi-scale context features\n",
        "        ctx_scales = [ctx]\n",
        "        for i in range(2):  # Generate 3 scales\n",
        "            ctx_scaled = F.adaptive_avg_pool1d(ctx.permute(0, 2, 1), ctx.size(1) // (2 ** (i+1)))\n",
        "            ctx_scaled = F.interpolate(ctx_scaled, size=ctx.size(1), mode='linear')\n",
        "            ctx_scales.append(ctx_scaled.permute(0, 2, 1))\n",
        "\n",
        "        ctx = torch.stack(ctx_scales).mean(dim=0)\n",
        "        ctx = ctx.unsqueeze(0).expand(batch_size * self.n_classes, -1, -1)\n",
        "\n",
        "        name_embeddings = self.embedding[:, self.prompt_prefix_length:, :]\n",
        "        name_embeddings = name_embeddings.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
        "        name_embeddings = name_embeddings.reshape(-1, self.name_length, self.ctx_dim)\n",
        "\n",
        "        prompts = torch.cat([ctx, name_embeddings], dim=1)\n",
        "\n",
        "        # Handle prompt length\n",
        "        if prompts.size(1) > 77:\n",
        "            prompts = prompts[:, :77, :]\n",
        "        else:\n",
        "            padding = torch.zeros(prompts.size(0), 77 - prompts.size(1), prompts.size(2),\n",
        "                                dtype=self.dtype, device=self.device)\n",
        "            prompts = torch.cat([prompts, padding], dim=1)\n",
        "\n",
        "        return prompts"
      ],
      "metadata": {
        "id": "bCYh7ItO1uuk"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, classnames, clip_model, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.dtype = clip_model.dtype\n",
        "        self.prompt_learner = PromptLearner(classnames, clip_model, device)\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = clip_model.transformer\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "\n",
        "        # Improved Feature Fusion with Attention\n",
        "        self.fusion = nn.ModuleDict({\n",
        "            'attention': nn.MultiheadAttention(\n",
        "                clip_model.visual.output_dim,\n",
        "                num_heads=8,\n",
        "                dropout=0.1\n",
        "            ),\n",
        "            'norm1': nn.LayerNorm(clip_model.visual.output_dim),\n",
        "            'norm2': nn.LayerNorm(clip_model.visual.output_dim),\n",
        "            'mlp': nn.Sequential(\n",
        "                nn.Linear(clip_model.visual.output_dim, clip_model.visual.output_dim * 4),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(clip_model.visual.output_dim * 4, clip_model.visual.output_dim),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "        })\n",
        "\n",
        "    def forward(self, image):\n",
        "        batch_size = image.size(0)\n",
        "        image = image.to(self.dtype)\n",
        "\n",
        "        # Extract image features with gradient checkpointing\n",
        "        with torch.cuda.amp.autocast():\n",
        "            image_features = self.image_encoder(image)\n",
        "\n",
        "        # Apply improved feature fusion with attention\n",
        "        image_features = image_features.unsqueeze(0)  # Add sequence dimension for attention\n",
        "        attn_output, _ = self.fusion['attention'](\n",
        "            image_features, image_features, image_features\n",
        "        )\n",
        "        image_features = image_features + attn_output  # Residual connection\n",
        "        image_features = self.fusion['norm1'](image_features)\n",
        "\n",
        "        # MLP with residual connection\n",
        "        mlp_output = self.fusion['mlp'](image_features)\n",
        "        image_features = image_features + mlp_output\n",
        "        image_features = self.fusion['norm2'](image_features)\n",
        "\n",
        "        image_features = image_features.squeeze(0)  # Remove sequence dimension\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        prompts = self.prompt_learner(batch_size)\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.text_encoder(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        text_features = x[:, -1, :] @ self.text_projection\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        image_features = image_features.unsqueeze(1).expand(-1, self.prompt_learner.n_classes, -1)\n",
        "        image_features = image_features.reshape(-1, image_features.size(-1))\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * torch.sum(image_features * text_features, dim=-1)\n",
        "        logits = logits.view(batch_size, self.prompt_learner.n_classes)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Rn0N_HJt1uqB"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PersonalColorDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_paths: List[str],\n",
        "        labels: List[int],\n",
        "        transform: Optional[transforms.Compose] = None,\n",
        "        augment: bool = False\n",
        "    ):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "\n",
        "        # Improvement 5: Enhanced color augmentation\n",
        "        if augment:\n",
        "            self.color_aug = transforms.Compose([\n",
        "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "                transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
        "                transforms.RandomAutocontrast(p=0.5)\n",
        "            ])\n",
        "        else:\n",
        "            self.color_aug = None\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            if self.augment and np.random.random() > 0.5:\n",
        "                image = self.color_aug(image)\n",
        "\n",
        "        return image, self.labels[idx]"
      ],
      "metadata": {
        "id": "il-IhNN_7szL"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "    # Fixed optimizer with non-overlapping parameter groups\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': [p for n, p in model.prompt_learner.named_parameters() if \"meta_net\" not in n], 'lr': 2e-3},\n",
        "        {'params': model.prompt_learner.meta_net.parameters(), 'lr': 1e-3},\n",
        "        {'params': model.fusion.parameters(), 'lr': 5e-4}\n",
        "    ], weight_decay=0.05)\n",
        "\n",
        "    # Improved learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=num_epochs // 3,\n",
        "        T_mult=2,\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    # Improved loss function with label smoothing\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation with model ensemble\n",
        "        val_acc = validate_with_tta(model, val_loader, device)"
      ],
      "metadata": {
        "id": "1pPnqZh_69Sa"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_with_tta(model, val_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Test-time augmentation\n",
        "            tta_outputs = []\n",
        "            for flip in [False, True]:\n",
        "                img = images.flip(3) if flip else images\n",
        "                for scale in [0.9, 1.0, 1.1]:\n",
        "                    size = int(224 * scale)\n",
        "                    if size != 224:\n",
        "                        img_scaled = F.interpolate(img, size=(size, size), mode='bilinear')\n",
        "                        img_scaled = F.interpolate(img_scaled, size=(224, 224), mode='bilinear')\n",
        "                    else:\n",
        "                        img_scaled = img\n",
        "\n",
        "                    outputs = model(img_scaled)\n",
        "                    tta_outputs.append(outputs)\n",
        "\n",
        "            # Average predictions\n",
        "            outputs = torch.stack(tta_outputs).mean(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "nHuM38U7BOxa"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    device: torch.device\n",
        ") -> Tuple[float, str, np.ndarray]:\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader, desc='Evaluating'):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                logits = model(images)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    class_names = ['spring', 'summer', 'fall', 'winter']\n",
        "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, report, conf_matrix"
      ],
      "metadata": {
        "id": "sHX3llk11ukK"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    dataset_dir = '/content/drive/Othercomputers/내 노트북/personal-color-data/'\n",
        "    dataset_types = ['train', 'test']\n",
        "    class_folders = ['spring', 'summer', 'fall', 'winter']\n",
        "\n",
        "    image_paths = {'train': [], 'test': []}\n",
        "    labels = {'train': [], 'test': []}\n",
        "\n",
        "    for dataset_type in dataset_types:\n",
        "        for idx, class_folder in enumerate(class_folders):\n",
        "            class_dir = os.path.join(dataset_dir, dataset_type, class_folder)\n",
        "            for img_path in glob.glob(os.path.join(class_dir, '*.*')):\n",
        "                if img_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_paths[dataset_type].append(img_path)\n",
        "                    labels[dataset_type].append(idx)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "    model = CustomCLIP(class_folders, clip_model, device).to(device)\n",
        "\n",
        "    # 개선 9: Strong augmentation for training\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.RandomApply([transforms.ColorJitter(0.3, 0.3, 0.3, 0.1)], p=0.5),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                           std=(0.26862954, 0.26130258, 0.27577711))\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                           std=(0.26862954, 0.26130258, 0.27577711))\n",
        "    ])\n",
        "\n",
        "    train_dataset = PersonalColorDataset(image_paths['train'], labels['train'],\n",
        "                                       transform=train_transform, augment=True)\n",
        "    val_dataset = PersonalColorDataset(image_paths['test'], labels['test'],\n",
        "                                     transform=val_transform, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                            num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, num_epochs=15, device=device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "_xQv8I5T1ufE",
        "outputId": "83511c0e-201b-4cb1-fac9-268702f805d0"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-85-cba8e138ea8c>:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-83-2bd2b1230d61>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-7fb4c9005d70>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-7fb4c9005d70>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                           num_workers=4, pin_memory=True)\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-cba8e138ea8c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-2bd2b1230d61>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-edbbd4044c2a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mctx_scales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Generate 3 scales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mctx_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mctx_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mctx_scales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
          ]
        }
      ]
    }
  ]
}