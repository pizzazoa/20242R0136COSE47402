!pip install git+https://github.com/openai/CLIP.git
!pip install ftfy regex tqdm

import torch
import torch.nn as nn
import clip
from PIL import Image
import os
import glob
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter

class PromptLearner(nn.Module):
    def __init__(self, classnames, clip_model, device):
        super().__init__()
        self.device = device
        self.n_classes = len(classnames)
        self.ctx_dim = clip_model.ln_final.weight.shape[0]
        self.n_ctx = 16
        self.dtype = clip_model.dtype

        # 학습 가능한 컨텍스트 토큰 초기화
        self.ctx = nn.Parameter(torch.empty(self.n_ctx, self.ctx_dim, dtype=self.dtype, device=device))
        nn.init.normal_(self.ctx, std=0.02)

        # 클래스 이름 토큰화 및 임베딩
        self.classnames = classnames
        self.tokenized_prompts = torch.cat([clip.tokenize(f"a photo of {name}") for name in classnames]).to(device)

        with torch.no_grad():
            embedding = clip_model.token_embedding(self.tokenized_prompts).type(self.dtype)

        self.register_buffer('embedding', embedding)
        self.prompt_prefix_length = 4
        self.name_length = embedding.size(1) - self.prompt_prefix_length
        self.total_length = self.n_ctx + self.name_length

    def forward(self, batch_size):
        ctx = self.ctx.unsqueeze(0).expand(batch_size * self.n_classes, -1, -1)

        # 클래스 이름 부분만 추출
        name_embeddings = self.embedding[:, self.prompt_prefix_length:, :]
        name_embeddings = name_embeddings.unsqueeze(0).expand(batch_size, -1, -1, -1)
        name_embeddings = name_embeddings.reshape(-1, self.name_length, self.ctx_dim)

        # 프롬프트 생성
        prompts = torch.cat([ctx, name_embeddings], dim=1)

        # CLIP의 최대 토큰 길이(77)에 맞게 조정
        if prompts.size(1) > 77:
            prompts = prompts[:, :77, :]
        else:
            padding_length = 77 - prompts.size(1)
            padding = torch.zeros(prompts.size(0), padding_length, prompts.size(2),
                                dtype=self.dtype, device=self.device)
            prompts = torch.cat([prompts, padding], dim=1)

        return prompts

class CustomCLIP(nn.Module):
    def __init__(self, classnames, clip_model, device):
        super().__init__()
        self.device = device
        self.dtype = clip_model.dtype
        self.prompt_learner = PromptLearner(classnames, clip_model, device)
        self.image_encoder = clip_model.visual
        self.text_encoder = clip_model.transformer
        self.text_projection = clip_model.text_projection
        self.logit_scale = clip_model.logit_scale
        self.positional_embedding = clip_model.positional_embedding
        self.ln_final = clip_model.ln_final
        self.token_embedding = clip_model.token_embedding

    def forward(self, image):
        batch_size = image.size(0)

        # 이미지를 올바른 dtype으로 변환
        image = image.to(self.dtype)

        # 이미지 특징 추출
        image_features = self.image_encoder(image)
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)

        # 프롬프트 생성
        prompts = self.prompt_learner(batch_size)
        x = prompts + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)

        # 텍스트 인코딩
        x = self.text_encoder(x)
        x = x.permute(1, 0, 2)
        x = self.ln_final(x)

        # 텍스트 특징 추출
        text_features = x[:, -1, :]
        text_features = text_features @ self.text_projection
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)

        # 이미지 특징과 텍스트 특징 매칭
        image_features = image_features.unsqueeze(1).expand(-1, self.prompt_learner.n_classes, -1)
        image_features = image_features.reshape(-1, image_features.size(-1))

        # 로짓 계산
        logit_scale = self.logit_scale.exp()
        logits = logit_scale * torch.sum(image_features * text_features, dim=-1)
        logits = logits.view(batch_size, self.prompt_learner.n_classes)

        return logits

def main():
    # 데이터 준비
    dataset_dir = '/content/drive/Othercomputers/내 노트북/personal-color-data/'
    dataset_types = ['train', 'test']
    class_folders = ['spring', 'summer', 'fall', 'winter']

    # 이미지 경로와 레이블 준비
    image_paths = {'train': [], 'test': []}
    labels = {'train': [], 'test': []}

    for dataset_type in dataset_types:
        for idx, class_folder in enumerate(class_folders):
            class_dir = os.path.join(dataset_dir, dataset_type, class_folder)
            for img_path in glob.glob(os.path.join(class_dir, '*.*')):
                if img_path.lower().endswith(('.jpg', '.jpeg', '.png')):
                    image_paths[dataset_type].append(img_path)
                    labels[dataset_type].append(idx)

    # 디바이스 설정
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # CLIP 모델 로드
    clip_model, _ = clip.load("ViT-B/32", device=device)

    # 커스텀 CLIP 모델 초기화
    model = CustomCLIP(class_folders, clip_model, device).to(device)

    # 데이터셋 및 데이터로더 설정
    class PersonalColorDataset(Dataset):
        def __init__(self, image_paths, labels, transform=None):
            self.image_paths = image_paths
            self.labels = labels
            self.transform = transform

        def __len__(self):
            return len(self.image_paths)

        def __getitem__(self, idx):
            image = Image.open(self.image_paths[idx]).convert('RGB')
            if self.transform:
                image = self.transform(image)
            label = self.labels[idx]
            return image, label

    transform = transforms.Compose([
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),
                           std=(0.26862954, 0.26130258, 0.27577711))
    ])

    train_dataset = PersonalColorDataset(image_paths['train'], labels['train'], transform)
    val_dataset = PersonalColorDataset(image_paths['test'], labels['test'], transform)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

    # CLIP 모델의 파라미터 고정
    for name, param in model.named_parameters():
        if not name.startswith('prompt_learner'):
            param.requires_grad = False

    # 옵티마이저 설정
    optimizer = torch.optim.AdamW(model.prompt_learner.parameters(), lr=1e-3, weight_decay=0.001)
    num_epochs = 10
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    # 클래스 가중치 계산 및 손실 함수 설정
    label_counts = Counter(labels['train'])
    total_samples = sum(label_counts.values())
    class_weights = [total_samples / label_counts[i] for i in range(len(class_folders))]
    class_weights = torch.tensor(class_weights, dtype=model.dtype, device=device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # 학습 루프
    best_val_acc = 0.0
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0

        for images, labels in tqdm(train_loader):
            images = images.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()

            # 데이터 타입 일치시키기
            logits = model(images)
            loss = criterion(logits, labels)

            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # 평가
        val_acc, val_report, val_conf_matrix = evaluate_model(model, val_loader, device)
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"Training Loss: {train_loss/len(train_loader):.4f}")
        print(f"Validation Accuracy: {val_acc:.4f}")
        print("\nClassification Report:")
        print(val_report)

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_cocoop_model.pth')

        scheduler.step()

def evaluate_model(model, data_loader, device):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in tqdm(data_loader):
            images = images.to(device)
            labels = labels.to(device)

            logits = model(images)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    class_names = ['spring', 'summer', 'fall', 'winter']
    report = classification_report(all_labels, all_preds, target_names=class_names)
    conf_matrix = confusion_matrix(all_labels, all_preds)

    return accuracy, report, conf_matrix

if __name__ == "__main__":
    main()