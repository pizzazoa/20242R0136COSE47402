{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gUM4_FgyC2d",
        "outputId": "de8036b5-90de-41b6-d1c4-3c266dffee84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Personal Color 분류 CLIP 2 - FineTuning\n",
        "\n",
        "# 라이브러리 임포트\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import clip\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS7Hq3W_3AOJ",
        "outputId": "cdac69e1-3d74-4a88-f9e7-0cf773186d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-cuironei\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-cuironei\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=29b11edf1c7e8f5f01c189f5933b3b7c9efd1652df715cf50d2276a987b2324d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uxu2f92p/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PersonalColorDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        image = image.float()\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "lQuYWVLrQu6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPFineTune(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CLIPFineTune, self).__init__()\n",
        "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=\"cuda\", jit=False)\n",
        "        self.clip_model.float()\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "        # 마지막 레이어만 fine tuning\n",
        "        for name, param in self.clip_model.named_parameters():\n",
        "            if \"visual.transformer.resblocks.11\" not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, image):\n",
        "        features = self.clip_model.encode_image(image)\n",
        "        outputs = self.classifier(features)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ROHZ4TGAQuuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        # Train\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        for images, labels in tqdm(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_preds.extend(predicted.cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "        # Val\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(val_loader):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "        print(f'Training Loss: {running_loss/len(train_loader):.4f}')\n",
        "        print(f'Training Accuracy: {train_acc:.4f}')\n",
        "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print('\\nClassification Report (Validation):')\n",
        "        print(classification_report(val_labels, val_preds,\n",
        "                                 target_names=['spring', 'summer', 'fall', 'winter']))"
      ],
      "metadata": {
        "id": "6ySvvRimQuiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    dataset_dir = '/content/drive/Othercomputers/내 노트북/personal-color-data/'\n",
        "    dataset_types = ['train', 'test']\n",
        "    class_folders = ['spring', 'summer', 'fall', 'winter']\n",
        "\n",
        "    image_paths = {'train': [], 'test': []}\n",
        "    labels = {'train': [], 'test': []}\n",
        "\n",
        "    for dataset_type in dataset_types:\n",
        "        for idx, class_folder in enumerate(class_folders):\n",
        "            class_dir = os.path.join(dataset_dir, dataset_type, class_folder)\n",
        "            for img_path in glob.glob(os.path.join(class_dir, '*.*')):\n",
        "                if img_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_paths[dataset_type].append(img_path)\n",
        "                    labels[dataset_type].append(idx)\n",
        "\n",
        "    # 모델 선택\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Data loaders\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                           (0.26862954, 0.26130258, 0.27577711))\n",
        "    ])\n",
        "\n",
        "    train_dataset = PersonalColorDataset(image_paths['train'], labels['train'], transform)\n",
        "    val_dataset = PersonalColorDataset(image_paths['test'], labels['test'], transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = CLIPFineTune(num_classes=4).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Training\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                num_epochs=10, device=device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "VIXsLrbvSDmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509b7f6d-ca1f-4fba-a2dc-a263e501525f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 116MiB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [15:21<00:00, 11.38s/it]\n",
            "100%|██████████| 15/15 [03:23<00:00, 13.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.1467\n",
            "Training Accuracy: 0.4705\n",
            "Validation Accuracy: 0.5326\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.55      0.28      0.37       214\n",
            "      summer       0.45      0.57      0.50       189\n",
            "        fall       0.51      0.53      0.52       266\n",
            "      winter       0.61      0.71      0.66       268\n",
            "\n",
            "    accuracy                           0.53       937\n",
            "   macro avg       0.53      0.52      0.51       937\n",
            "weighted avg       0.54      0.53      0.52       937\n",
            "\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:30<00:00,  2.69it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8611\n",
            "Training Accuracy: 0.6456\n",
            "Validation Accuracy: 0.5155\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.43      0.38      0.40       214\n",
            "      summer       0.48      0.56      0.51       189\n",
            "        fall       0.49      0.52      0.50       266\n",
            "      winter       0.64      0.59      0.61       268\n",
            "\n",
            "    accuracy                           0.52       937\n",
            "   macro avg       0.51      0.51      0.51       937\n",
            "weighted avg       0.52      0.52      0.52       937\n",
            "\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:28<00:00,  2.81it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6802\n",
            "Training Accuracy: 0.7326\n",
            "Validation Accuracy: 0.5059\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.41      0.63      0.50       214\n",
            "      summer       0.60      0.31      0.41       189\n",
            "        fall       0.49      0.46      0.47       266\n",
            "      winter       0.60      0.59      0.60       268\n",
            "\n",
            "    accuracy                           0.51       937\n",
            "   macro avg       0.52      0.50      0.49       937\n",
            "weighted avg       0.53      0.51      0.50       937\n",
            "\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:30<00:00,  2.69it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5003\n",
            "Training Accuracy: 0.8229\n",
            "Validation Accuracy: 0.5368\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.51      0.47      0.49       214\n",
            "      summer       0.50      0.49      0.49       189\n",
            "        fall       0.50      0.48      0.49       266\n",
            "      winter       0.62      0.68      0.65       268\n",
            "\n",
            "    accuracy                           0.54       937\n",
            "   macro avg       0.53      0.53      0.53       937\n",
            "weighted avg       0.53      0.54      0.53       937\n",
            "\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:30<00:00,  2.61it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3258\n",
            "Training Accuracy: 0.9060\n",
            "Validation Accuracy: 0.5165\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.49      0.39      0.43       214\n",
            "      summer       0.45      0.56      0.50       189\n",
            "        fall       0.50      0.43      0.46       266\n",
            "      winter       0.59      0.68      0.63       268\n",
            "\n",
            "    accuracy                           0.52       937\n",
            "   macro avg       0.51      0.51      0.51       937\n",
            "weighted avg       0.51      0.52      0.51       937\n",
            "\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:28<00:00,  2.85it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1955\n",
            "Training Accuracy: 0.9542\n",
            "Validation Accuracy: 0.5155\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.46      0.52      0.49       214\n",
            "      summer       0.52      0.39      0.45       189\n",
            "        fall       0.47      0.60      0.53       266\n",
            "      winter       0.65      0.52      0.58       268\n",
            "\n",
            "    accuracy                           0.52       937\n",
            "   macro avg       0.52      0.51      0.51       937\n",
            "weighted avg       0.53      0.52      0.52       937\n",
            "\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:28<00:00,  2.85it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1055\n",
            "Training Accuracy: 0.9842\n",
            "Validation Accuracy: 0.4973\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.45      0.46      0.46       214\n",
            "      summer       0.44      0.53      0.48       189\n",
            "        fall       0.49      0.39      0.44       266\n",
            "      winter       0.58      0.61      0.60       268\n",
            "\n",
            "    accuracy                           0.50       937\n",
            "   macro avg       0.49      0.50      0.49       937\n",
            "weighted avg       0.50      0.50      0.50       937\n",
            "\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:28<00:00,  2.84it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0585\n",
            "Training Accuracy: 0.9955\n",
            "Validation Accuracy: 0.4984\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.47      0.43      0.45       214\n",
            "      summer       0.44      0.55      0.49       189\n",
            "        fall       0.48      0.42      0.45       266\n",
            "      winter       0.59      0.59      0.59       268\n",
            "\n",
            "    accuracy                           0.50       937\n",
            "   macro avg       0.49      0.50      0.49       937\n",
            "weighted avg       0.50      0.50      0.50       937\n",
            "\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:28<00:00,  2.81it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0343\n",
            "Training Accuracy: 0.9992\n",
            "Validation Accuracy: 0.4771\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.45      0.29      0.35       214\n",
            "      summer       0.42      0.62      0.50       189\n",
            "        fall       0.45      0.47      0.46       266\n",
            "      winter       0.59      0.53      0.56       268\n",
            "\n",
            "    accuracy                           0.48       937\n",
            "   macro avg       0.48      0.48      0.47       937\n",
            "weighted avg       0.48      0.48      0.47       937\n",
            "\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:28<00:00,  2.83it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0467\n",
            "Training Accuracy: 0.9930\n",
            "Validation Accuracy: 0.5059\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.45      0.50      0.48       214\n",
            "      summer       0.47      0.50      0.49       189\n",
            "        fall       0.48      0.48      0.48       266\n",
            "      winter       0.62      0.53      0.57       268\n",
            "\n",
            "    accuracy                           0.51       937\n",
            "   macro avg       0.51      0.51      0.50       937\n",
            "weighted avg       0.51      0.51      0.51       937\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    dataset_dir = '/content/drive/Othercomputers/내 노트북/personal-color-data/'\n",
        "    dataset_types = ['train', 'test']\n",
        "    class_folders = ['spring', 'summer', 'fall', 'winter']\n",
        "\n",
        "    image_paths = {'train': [], 'test': []}\n",
        "    labels = {'train': [], 'test': []}\n",
        "\n",
        "    for dataset_type in dataset_types:\n",
        "        for idx, class_folder in enumerate(class_folders):\n",
        "            class_dir = os.path.join(dataset_dir, dataset_type, class_folder)\n",
        "            for img_path in glob.glob(os.path.join(class_dir, '*.*')):\n",
        "                if img_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_paths[dataset_type].append(img_path)\n",
        "                    labels[dataset_type].append(idx)\n",
        "\n",
        "    # 모델 선택\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Data loaders\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                           (0.26862954, 0.26130258, 0.27577711))\n",
        "    ])\n",
        "\n",
        "    train_dataset = PersonalColorDataset(image_paths['train'], labels['train'], transform)\n",
        "    val_dataset = PersonalColorDataset(image_paths['test'], labels['test'], transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = CLIPFineTune(num_classes=4).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Training\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                num_epochs=5, device=device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_QZQmPgRSTHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1cc53a5-1d37-45d2-c15d-7be883e137bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:36<00:00,  2.21it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.1403\n",
            "Training Accuracy: 0.4790\n",
            "Validation Accuracy: 0.5208\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.53      0.27      0.35       214\n",
            "      summer       0.43      0.71      0.54       189\n",
            "        fall       0.54      0.40      0.46       266\n",
            "      winter       0.60      0.71      0.65       268\n",
            "\n",
            "    accuracy                           0.52       937\n",
            "   macro avg       0.52      0.52      0.50       937\n",
            "weighted avg       0.53      0.52      0.51       937\n",
            "\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:30<00:00,  2.63it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8632\n",
            "Training Accuracy: 0.6491\n",
            "Validation Accuracy: 0.5496\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.55      0.29      0.38       214\n",
            "      summer       0.45      0.69      0.55       189\n",
            "        fall       0.59      0.42      0.49       266\n",
            "      winter       0.61      0.78      0.68       268\n",
            "\n",
            "    accuracy                           0.55       937\n",
            "   macro avg       0.55      0.55      0.53       937\n",
            "weighted avg       0.56      0.55      0.53       937\n",
            "\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:29<00:00,  2.77it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6775\n",
            "Training Accuracy: 0.7459\n",
            "Validation Accuracy: 0.4867\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.50      0.22      0.31       214\n",
            "      summer       0.41      0.68      0.51       189\n",
            "        fall       0.47      0.51      0.49       266\n",
            "      winter       0.61      0.54      0.57       268\n",
            "\n",
            "    accuracy                           0.49       937\n",
            "   macro avg       0.50      0.49      0.47       937\n",
            "weighted avg       0.50      0.49      0.48       937\n",
            "\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:30<00:00,  2.64it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4940\n",
            "Training Accuracy: 0.8325\n",
            "Validation Accuracy: 0.5304\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.51      0.47      0.49       214\n",
            "      summer       0.53      0.50      0.51       189\n",
            "        fall       0.49      0.47      0.48       266\n",
            "      winter       0.58      0.66      0.62       268\n",
            "\n",
            "    accuracy                           0.53       937\n",
            "   macro avg       0.53      0.53      0.53       937\n",
            "weighted avg       0.53      0.53      0.53       937\n",
            "\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:28<00:00,  2.80it/s]\n",
            "100%|██████████| 15/15 [00:05<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3311\n",
            "Training Accuracy: 0.8986\n",
            "Validation Accuracy: 0.4995\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      spring       0.47      0.50      0.48       214\n",
            "      summer       0.47      0.49      0.48       189\n",
            "        fall       0.46      0.48      0.47       266\n",
            "      winter       0.60      0.53      0.56       268\n",
            "\n",
            "    accuracy                           0.50       937\n",
            "   macro avg       0.50      0.50      0.50       937\n",
            "weighted avg       0.50      0.50      0.50       937\n",
            "\n"
          ]
        }
      ]
    }
  ]
}